---
title: '通过ollama在本地运行开源的通义千问qwenLM'
date: '2024-11-4'
tags: [""]
draft: false
summary:
---

最近公司在推广自己的大模型平台，我也就顺势研究了一番，包括RAG、精调、开源、多模态内容。

[2024阿里云云栖大会](https://yunqi.aliyun.com/2024/live?spm=5176.29615464.J_1575700760.3.6be54c8417PtqN) 有50%篇幅着重介绍大模型，我也受益匪浅。

发现 [ollama](https://ollama.com/download) 能很容易本地部署大模型，也能轻易运行通义的开源版本 qwen2.5.

下面是步骤：
1. 下载ollama  https://ollama.com/download
2. 命令行加载大模型 https://github.com/ollama/ollama?tab=readme-ov-file#model-library
3. 或者加载千问 https://github.com/QwenLM/Qwen2.5?tab=readme-ov-file#-run-locally
4. 这时候可以命令行直接问，或者restapi来请求接口


这里是千问的文档，讲的很好 https://qwen.readthedocs.io/zh-cn/latest/

# 关于我
国 wei (Eric)
[Github](https://github.com/ygweric)

# [扫码加入独立开发微信群-二维码经常更新](https://raw.githubusercontent.com/ygweric/ygweric.github.io/main/assets/qr-schedule-update/indenpendent_dev.png)

# 关注公众号 [开发副业](https://github.com/ygweric/ygweric.github.io/blob/main/assets/jinjing/wx_office_account_qr.png?raw=true)，闲谈代码人生
